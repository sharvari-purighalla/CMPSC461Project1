import re
from typing import List, Tuple, Any, Union
from ASTNodeDefs import *


# A type alias for clarity, representing parts of expressions
# that can be a full ASTNode or a simple token tuple (like for numbers/identifiers).
ExprType = Union[ASTNode, Tuple[str, Any]]

class Lexer:
    """
    The Lexer (also known as a tokenizer or scanner) is responsible for breaking
    the raw source code string into a stream of meaningful tokens.
    """
    def __init__(self, code: str) -> None:
        """
        Initializes the Lexer.

        Args:
            code: The source code string to be tokenized.
        """
        self.code = code
        self.tokens: List[Tuple[str, Any]] = []
        # A list of tuples where each tuple contains a token name and a regex pattern.
        # The order is important, as it determines matching priority.
        self.token_specs = [
            ('NUMBER',     r'\d+'),
            ('IDENTIFIER', r'[A-Za-z_][A-Za-z0-9_]*'),
            ('IF',         r'if'),
            ('ELSE',       r'else'),
            ('FOR',        r'for'),
            ('TO',         r'to'),
            ('PRINT',      r'print'),
            ('AND',        r'and'),
            ('OR',         r'or'),
            ('NOT',        r'not'),
            ('PLUS',       r'\+'),
            ('MINUS',      r'-'),
            ('MULTIPLY',   r'\*'),
            ('DIVIDE',     r'/'),
            ('MODULO',     r'%'),
            ('EQ',         r'=='),
            ('NEQ',        r'!='),
            ('GREATER',    r'>'),
            ('LESS',       r'<'),
            ('EQUALS',     r'='),
            ('LPAREN',     r'\('),
            ('RPAREN',     r'\)'),
            ('COMMA',      r','),
            ('COLON',      r':'),
            ('SKIP',       r'[ \t\n]+'),  # Skips whitespace and newlines
            ('MISMATCH',   r'.'),         # Catches any other character
        ]
        # A single, combined regex for efficient matching.
        self.token_regex = '|'.join(f'(?P<{name}>{regex})' for name, regex in self.token_specs)

    # TODO: Implement this function
    def tokenize(self) -> List[Tuple[str, Any]]:
        """
        Why this function is needed: This is the core of the lexer. Its purpose is to
        transform the raw text-based source code into a structured list of tokens
        that the parser can understand. The parser cannot work with a raw string;
        it needs a sequence of categorized symbols.

        What this function does: It scans the input code from left to right, matching
        the text against the regular expressions defined in `self.token_specs`.
        For each match, it creates a (token_type, value) tuple and adds it to a list.
        It should handle converting numbers to integer types and correctly identifying
        keywords vs. identifiers. It discards meaningless characters like whitespace.
        The process ends when the entire code string is consumed, at which point an
        'EOF' (End of File) token is appended to signify the end of the input.
        """
        list_token = []

        for i in re.finditer(self.token_regex, self.code):

            value = i.group()
            type_of_value = i.lastgroup

            if type_of_value == "SKIP":
                continue

            elif type_of_value == "NUMBER":
                list_token.append((type_of_value,int(value)))

            elif type_of_value == "IDENTIFIER":
                key = ['IF', 'ELSE', 'FOR', 'TO','PRINT', 'AND', 'OR', 'NOT']
                if value in key:
                    list_token.append((key[value],value))
                else:
                    list_token.append((type_of_value,value))

            elif type_of_value == "MISMATCH":
                raise SyntaxError(
                    f"Unexpected token '{list_token}'"
                    "Expected 'NUMBER', 'IDENTIFIER', or other."
                )
            else:
                list_token.append((type_of_value,value))

        list_token.append(('EOF',None))

        return list_token


class Parser:
    """
    The Parser takes the list of tokens generated by the Lexer and builds an
    Abstract Syntax Tree (AST). The AST is a tree representation of the source
    code's structure that is much easier to work with for later stages like
    interpretation or compilation.
    """
    def __init__(self, tokens: List[Tuple[str, Any]]) -> None:
        """
        Initializes the Parser.

        Args:
            tokens: A list of tokens from the Lexer.
        """
        self.tokens = tokens
        self.pos = 0  # The parser's current position in the token stream

    def current_token(self) -> Tuple[str, Any]:
        """A helper function to look at the current token without consuming it."""
        return self.tokens[self.pos]

    def advance(self) -> None:
        """A helper function to consume the current token and move to the next one."""
        self.pos += 1

    def expect(self, kind: str) -> Tuple[str, Any]:
        """
        Checks if the current token matches an expected type. If so, it consumes
        the token. If not, it raises a syntax error. This is crucial for
        enforcing the language's grammar rules.
        """
        if self.current_token()[0] == kind:
            token = self.current_token()
            self.advance()
            return token
        else:
            raise SyntaxError(f"Expected {kind} but got {self.current_token()[0]} at position {self.pos}")

    def parse(self) -> List[ASTNode]:
        """
        Why this function is needed: This is the main entry point for the parsing process.
        It orchestrates the entire parsing operation by repeatedly parsing the fundamental
        unit of our language: a statement.

        What this function does: It creates an empty list to hold the statements of the
        program. It then loops as long as it has not reached the 'EOF' token. In each
        iteration, it calls `parse_statement()` to parse a single statement and appends
        the resulting AST node to the list. Finally, it returns the list of statement
        nodes, which represents the complete program.
        """
        statements = []
        while self.current_token()[0] != 'EOF':
            statements.append(self.parse_statement())
        return statements

    # TODO: Implement this function
    def parse_statement(self) -> ASTNode:
        """
        Why this function is needed: Our language is composed of different kinds of statements
        (assignments, if-statements, loops, etc.). This function acts as a dispatcher. It needs
        to figure out which kind of statement is next in the token stream and call the
        correct specific function to handle it.

        What this function does: It looks at the type of the `current_token`.
        - If it's an 'IDENTIFIER', it's likely an assignment (e.g., `x = ...`).
        - If it's an 'IF', it calls `parse_if_stmt()`.
        - If it's a 'FOR', it calls `parse_for_stmt()`.
        - If it's a 'PRINT', it calls `parse_print_stmt()`.
        This routing is the essence of a top-down recursive descent parser.
        """
        token_type = self.current_token()
        
        if token_type == 'IF':
            return self.parse_if_stmt()
        elif token_type == 'FOR':
            return self.parse_for_stmt()
        elif token_type == 'PRINT':
            return self.parse_print_stmt()
        elif token_type == 'IDENTIFIER':
            identifier_token = self.expect('IDENTIFIER')
            self.expect('EQUALS')
            expr = self.parse_expression()
            return ('ASSIGN', identifier_token[1], expr)
        
        else:
            raise SyntaxError(
                f"Unexpected token '{token_type}' at position {self.pos}. "
                "Expected 'IF', 'FOR', 'PRINT', or 'IDENTIFIER'."
            )
    


    # TODO: Implement this function
    def parse_if_stmt(self) -> IfStatement:
        """
        Why this function is needed: To parse the structure of an if-else statement according
        to the grammar rules.

        What this function does: It consumes the 'IF' token, then calls `parse_boolean_expression()`
        to parse the condition. It then expects and consumes a 'COLON', calls `parse_block()`
        to handle the body of the 'then' clause. After that, it checks for an 'ELSE' token to
        handle the optional else part, which also has a colon and a block. It constructs and
        returns an `IfStatement` AST node with the condition, then-block, and optional else-block.
        """
        self.expect('IF')
        condition = self.parse_boolean_expression()

        self.expect('COLON')
        then_block= self.parse_block()

        else_block=None

        if self.current_token()[0] == "ELSE":
            self.expect("ELSE")
            self.expect("COLON")
            else_block = self.parse_block

        return IfStatement(condition, then_block, else_block)

    # TODO: Implement this function
    def parse_for_stmt(self) -> ForStatement:
        """
        Why this function is needed: To parse the specific syntax of our for-loop.

        What this function does: It consumes tokens in the expected order for a for-loop:
        'FOR', an identifier for the loop variable, 'EQUALS', an expression for the start value,
        'TO', an expression for the end value, and a 'COLON'. Finally, it calls `parse_block()`
        for the loop's body. It bundles all this information into a `ForStatement` AST node.
        """
        pass

    # TODO: Implement this function
    def parse_print_stmt(self) -> PrintStatement:
        """
        Why this function is needed: To handle the language's built-in print statement.

        What this function does: It consumes the 'PRINT' token and an opening parenthesis 'LPAREN'.
        It then calls `parse_arg_list()` to parse the comma-separated expressions inside the
        parentheses. Finally, it consumes the closing parenthesis 'RPAREN' and returns a
        `PrintStatement` AST node containing the list of arguments.
        """
        pass

    # TODO: Implement this function
    def parse_block(self) -> Block:
        """
        Why this function is needed: Control flow statements like 'if' and 'for' need to execute
        a sequence of other statements. A 'block' represents this sequence.

        What this function does: It creates a list to hold statements. It then repeatedly calls
        `parse_statement()` to parse all statements until it reaches a token that signals the end
        of the block (in our simplified language, 'ELSE' or the end of the file). It returns a
        `Block` AST node containing the list of parsed statements.
        """
        pass

    # TODO: Implement this function
    def parse_arg_list(self) -> List[ExprType]:
        """
        Why this function is needed: To handle comma-separated lists of values, such as in the
        print statement.

        What this function does: It first parses one expression. Then, it enters a loop that
        continues as long as the current token is a 'COMMA'. Inside the loop, it consumes the
        comma and parses the next expression. It returns a list of all the parsed expression nodes.
        """
        pass

    # TODO: Implement this function
    def parse_boolean_expression(self) -> ExprType:
        """
        Why this function is needed: This function handles the logical 'OR' operator. To correctly
        implement operator precedence, we need a separate function for each level of precedence.
        'OR' has the lowest precedence among logical operators.

        What this function does: It first calls `parse_boolean_term()` to get the left-hand side.
        Then, it loops as long as it sees an 'OR' token. In the loop, it creates a `LogicalOperation`
        node with the left side, the 'OR' operator, and the result of parsing the right side.
        This left-associative structure correctly handles chains like `A or B or C`.
        """
        pass

    # TODO: Implement this function
    def parse_boolean_term(self) -> ExprType:
        """
        Why this function is needed: This handles the 'AND' operator, which has a higher
        precedence than 'OR'.

        What this function does: Its structure is identical to `parse_boolean_expression`, but
        it deals with the 'AND' operator and calls `parse_boolean_factor()` for its operands.
        This ensures that expressions like `A and B or C` are parsed as `(A and B) or C`.
        """
        pass

    # TODO: Implement this function
    def parse_boolean_factor(self) -> ExprType:
        """
        Why this function is needed: This handles the unary 'NOT' operator, which has the
        highest logical precedence.

        What this function does: It checks for a 'NOT' token. If found, it consumes it,
        recursively calls `parse_boolean_factor()` to parse the expression being negated, and
        wraps it in a `UnaryOperation` node. If there is no 'NOT', it simply calls the next
        level of the precedence hierarchy, `parse_comparison()`.
        """
        pass

    # TODO: Implement this function
    def parse_comparison(self) -> ExprType:
        """
        Why this function is needed: To parse comparison expressions like `a > b` or `x == 10`.

        What this function does: It first calls `parse_expression()` to get the left-hand side
        (an arithmetic expression). It then checks if the current token is a comparison
        operator ('==', '!=', '>', '<'). If so, it consumes the operator and calls
        `parse_expression()` again for the right-hand side, creating a `BinaryOperation` node.
        If not, it just returns the left-hand side node it already parsed.
        """
        pass

    # TODO: Implement this function
    def parse_expression(self) -> ExprType:
        """
        Why this function is needed: To handle the lowest precedence arithmetic operators:
        addition ('+') and subtraction ('-').

        What this function does: Following the same pattern as the boolean functions, it first
        calls `parse_term()` to get a higher-precedence operand. It then loops as long as it
        sees a 'PLUS' or 'MINUS' token, building `BinaryOperation` nodes in a left-associative way.
        """
        pass

    # TODO: Implement this function
    def parse_term(self) -> ExprType:
        """
        Why this function is needed: To handle multiplication ('*'), division ('/'), and modulo ('%'),
        which have higher precedence than addition and subtraction.

        What this function does: It calls `parse_factor()` to get its operands and loops on
        '*', '/', and '%' operators. This ensures that `a + b * c` is correctly parsed as `a + (b * c)`.
        """
        pass

    # TODO: Implement this function
    def parse_factor(self) -> ExprType:
        """
        Why this function is needed: To handle unary plus and minus operators (e.g., `-5`).
        These have higher precedence than multiplication.

        What this function does: It checks for a 'PLUS' or 'MINUS' token. If found, it consumes it,
        recursively calls `parse_factor()` for the operand, and returns a `UnaryOperation` node.
        If not, it calls `parse_primary()` for the highest-precedence elements.
        """
        pass

    # TODO: Implement this function
    def parse_primary(self) -> ExprType:
        """
        Why this function is needed: This function is at the bottom of the expression parsing
        hierarchy. It handles the most basic, highest-precedence elements of expressions. These
        are the "atoms" of an expression.

        What this function does: It checks for three cases:
        1. A 'NUMBER' token.
        2. An 'IDENTIFIER' token (a variable).
        3. An opening parenthesis 'LPAREN'. If found, it recursively calls `parse_boolean_expression()`
           to parse the entire expression inside the parentheses, and then expects a closing 'RPAREN'.
           This allows for manually overriding operator precedence (e.g., `(a + b) * c`).
        """
        pass
